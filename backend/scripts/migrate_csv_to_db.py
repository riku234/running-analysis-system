"""
CSVãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç§»è¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python backend/scripts/migrate_csv_to_db.py \
        --csv-dir /path/to/csv/files \
        --csv1 çµ±è¨ˆãƒ¢ãƒ‡ãƒ«æ¤œè¨.csv \
        --csv2 ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°åˆ†é¡.csv \
        --csv3 ã‚³ãƒ¡ãƒ³ãƒˆç´ã¥ã‘.csv
"""

import os
import sys
import argparse
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv
from typing import Dict, List, Any, Optional
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ==========================================
# 1. Zå€¤å¤‰æ•°åã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸
# ==========================================

# æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç‰¹å®šã—ãŸè§’åº¦åã¨ã‚¤ãƒ™ãƒ³ãƒˆå
ANGLE_NAMES = {
    'ä½“å¹¹': 'ä½“å¹¹è§’åº¦',
    'ä½“å¹¹è§’åº¦': 'ä½“å¹¹è§’åº¦',
    'å·¦å¤§è…¿': 'å·¦å¤§è…¿è§’åº¦',
    'å·¦å¤§è…¿è§’åº¦': 'å·¦å¤§è…¿è§’åº¦',
    'å³å¤§è…¿': 'å³å¤§è…¿è§’åº¦',
    'å³å¤§è…¿è§’åº¦': 'å³å¤§è…¿è§’åº¦',
    'å·¦ä¸‹è…¿': 'å·¦ä¸‹è…¿è§’åº¦',
    'å·¦ä¸‹è…¿è§’åº¦': 'å·¦ä¸‹è…¿è§’åº¦',
    'å³ä¸‹è…¿': 'å³ä¸‹è…¿è§’åº¦',
    'å³ä¸‹è…¿è§’åº¦': 'å³ä¸‹è…¿è§’åº¦',
}

EVENT_NAMES = {
    'å³è¶³æ¥åœ°': 'right_strike',
    'å³è¶³é›¢åœ°': 'right_off',
    'å·¦è¶³æ¥åœ°': 'left_strike',
    'å·¦è¶³é›¢åœ°': 'left_off',
    'right_strike': 'right_strike',
    'right_off': 'right_off',
    'left_strike': 'left_strike',
    'left_off': 'left_off',
}

OPERATOR_MAPPING = {
    'ã‚ˆã‚Šå°ã•ã„': 'lt',
    'ã‚ˆã‚Šå¤§ãã„': 'gt',
    'ä»¥ä¸‹': 'lte',
    'ä»¥ä¸Š': 'gte',
    'ç­‰ã—ã„': 'eq',
    'lt': 'lt',
    'gt': 'gt',
    'lte': 'lte',
    'gte': 'gte',
    'eq': 'eq',
    '<': 'lt',
    '>': 'gt',
    '<=': 'lte',
    '>=': 'gte',
    '=': 'eq',
}

# ==========================================
# 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
# ==========================================

def get_db_connection():
    """
    PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šã‚’ç¢ºç«‹
    ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‹ã‚‰å®Ÿè¡Œã™ã‚‹å ´åˆã€ãƒ›ã‚¹ãƒˆåã‚’è‡ªå‹•çš„ã«èª¿æ•´
    """
    db_host = os.getenv("DB_HOST")
    db_port = os.getenv("DB_PORT", "5432")
    db_name = os.getenv("DB_NAME", "app")
    db_user = os.getenv("DB_USER", "postgres")
    db_password = os.getenv("DB_PASSWORD", "postgres")
    
    if not db_host:
        # DB_HOSTãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‚’æƒ³å®š
        db_host = "localhost"
        print("âš ï¸  DB_HOSTãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚localhostã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    # æ¥ç¶šã‚’è©¦è¡Œã™ã‚‹ãƒ›ã‚¹ãƒˆåã®ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    hosts_to_try = [db_host]
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‹ã‚‰å®Ÿè¡Œã™ã‚‹å ´åˆã€Dockerã‚³ãƒ³ãƒ†ãƒŠåï¼ˆdbï¼‰ã§ã¯æ¥ç¶šã§ããªã„
    # ãã®ãŸã‚ã€localhostã‚‚è©¦è¡Œã™ã‚‹
    if db_host == "db":
        hosts_to_try.append("localhost")
        print(f"ğŸ’¡ Dockerã‚³ãƒ³ãƒ†ãƒŠå 'db' ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã™ã€‚")
        print(f"   ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‹ã‚‰å®Ÿè¡Œã™ã‚‹å ´åˆã€'localhost' ã‚‚è©¦è¡Œã—ã¾ã™ã€‚")
    
    # å„ãƒ›ã‚¹ãƒˆåã§æ¥ç¶šã‚’è©¦è¡Œ
    last_error = None
    for host in hosts_to_try:
        try:
            print(f"ğŸ”Œ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’è©¦ã¿ã¦ã„ã¾ã™...")
            print(f"   ãƒ›ã‚¹ãƒˆ: {host}")
            print(f"   ãƒãƒ¼ãƒˆ: {db_port}")
            print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {db_name}")
            print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼: {db_user}")
            
            connection = psycopg2.connect(
                host=host,
                port=db_port,
                database=db_name,
                user=db_user,
                password=db_password,
                connect_timeout=10
            )
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ! (ãƒ›ã‚¹ãƒˆ: {host})")
            return connection
        except psycopg2.OperationalError as e:
            last_error = e
            print(f"âš ï¸  ãƒ›ã‚¹ãƒˆ '{host}' ã¸ã®æ¥ç¶šã«å¤±æ•—: {e}")
            if host != hosts_to_try[-1]:
                print(f"   æ¬¡ã®ãƒ›ã‚¹ãƒˆã‚’è©¦è¡Œã—ã¾ã™...")
                continue
        except Exception as e:
            last_error = e
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            break
    
    # ã™ã¹ã¦ã®æ¥ç¶šè©¦è¡ŒãŒå¤±æ•—ã—ãŸå ´åˆ
    print(f"\nâŒ ã™ã¹ã¦ã®æ¥ç¶šè©¦è¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
    print(f"\nğŸ’¡ ç¢ºèªäº‹é …:")
    print(f"   1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹:")
    print(f"      docker compose ps db")
    print(f"   2. .envãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®šã‚’ç¢ºèª:")
    print(f"      DB_HOST=localhost (ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ)")
    print(f"      ã¾ãŸã¯ DB_HOST=db (Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã‹ã‚‰å®Ÿè¡Œã™ã‚‹å ´åˆ)")
    print(f"   3. ãƒãƒ¼ãƒˆ5432ãŒä½¿ç”¨å¯èƒ½ã‹:")
    print(f"      lsof -i :5432")
    
    raise ConnectionError(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {last_error}")

# ==========================================
# 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
# ==========================================

def create_tables(conn):
    """è¨ºæ–­ãƒ«ãƒ¼ãƒ«ã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
    cursor = conn.cursor()
    
    # è¨ºæ–­ãƒ«ãƒ¼ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS diagnosis_rules (
            id SERIAL PRIMARY KEY,
            rule_code VARCHAR(100) UNIQUE NOT NULL,
            rule_name VARCHAR(255) NOT NULL,
            target_event VARCHAR(50),
            -- 'right_strike', 'right_off', 'left_strike', 'left_off', NULL (å…¨ã‚¤ãƒ™ãƒ³ãƒˆ)
            target_metric VARCHAR(100) NOT NULL,
            -- 'ä½“å¹¹è§’åº¦', 'å·¦å¤§è…¿è§’åº¦', 'å³å¤§è…¿è§’åº¦', 'å·¦ä¸‹è…¿è§’åº¦', 'å³ä¸‹è…¿è§’åº¦'
            operator VARCHAR(10) NOT NULL,
            -- 'lt', 'gt', 'lte', 'gte', 'eq'
            threshold FLOAT NOT NULL,
            severity VARCHAR(20) NOT NULL,
            -- 'high', 'medium', 'low'
            priority INTEGER DEFAULT 0,
            is_active BOOLEAN DEFAULT TRUE,
            created_at TIMESTAMP DEFAULT NOW(),
            updated_at TIMESTAMP DEFAULT NOW()
        );
    """)
    
    # å°‚é–€å®¶ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS expert_advice (
            id SERIAL PRIMARY KEY,
            rule_code VARCHAR(100) NOT NULL,
            issue_name VARCHAR(255) NOT NULL,
            observation TEXT,
            cause TEXT,
            action TEXT,
            drill_name VARCHAR(255),
            drill_url VARCHAR(500),
            additional_notes TEXT,
            created_at TIMESTAMP DEFAULT NOW(),
            updated_at TIMESTAMP DEFAULT NOW(),
            FOREIGN KEY (rule_code) REFERENCES diagnosis_rules(rule_code) ON DELETE CASCADE
        );
    """)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_diagnosis_rules_code ON diagnosis_rules(rule_code);
        CREATE INDEX IF NOT EXISTS idx_diagnosis_rules_metric ON diagnosis_rules(target_metric);
        CREATE INDEX IF NOT EXISTS idx_diagnosis_rules_event ON diagnosis_rules(target_event);
        CREATE INDEX IF NOT EXISTS idx_expert_advice_rule_code ON expert_advice(rule_code);
    """)
    
    conn.commit()
    cursor.close()
    print("âœ… ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†!")

# ==========================================
# 4. CSVãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æ­£è¦åŒ–
# ==========================================

def normalize_angle_name(angle_str: str) -> Optional[str]:
    """è§’åº¦åã‚’æ­£è¦åŒ–"""
    if pd.isna(angle_str) or not angle_str:
        return None
    angle_str = str(angle_str).strip()
    return ANGLE_NAMES.get(angle_str, angle_str)

def normalize_event_name(event_str: str) -> Optional[str]:
    """ã‚¤ãƒ™ãƒ³ãƒˆåã‚’æ­£è¦åŒ–"""
    if pd.isna(event_str) or not event_str:
        return None
    event_str = str(event_str).strip()
    return EVENT_NAMES.get(event_str, event_str)

def normalize_operator(operator_str: str) -> Optional[str]:
    """æ¼”ç®—å­ã‚’æ­£è¦åŒ–"""
    if pd.isna(operator_str) or not operator_str:
        return None
    operator_str = str(operator_str).strip()
    return OPERATOR_MAPPING.get(operator_str, operator_str)

def load_and_process_csv1(csv_path: str) -> pd.DataFrame:
    """
    çµ±è¨ˆãƒ¢ãƒ‡ãƒ«æ¤œè¨.csv ã‚’èª­ã¿è¾¼ã¿ã€è¨ºæ–­ãƒ«ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    
    æƒ³å®šã•ã‚Œã‚‹CSVæ§‹é€ :
    - ãƒ«ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ‰, ãƒ«ãƒ¼ãƒ«å, ã‚¤ãƒ™ãƒ³ãƒˆ, è§’åº¦, æ¼”ç®—å­, é–¾å€¤, é‡è¦åº¦, å„ªå…ˆåº¦
    """
    print(f"ğŸ“– CSV1ã‚’èª­ã¿è¾¼ã¿ä¸­: {csv_path}")
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    
    # ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã€ç©ºç™½ã‚’é™¤å»ï¼‰
    df.columns = df.columns.str.strip().str.lower()
    
    print(f"   ã‚«ãƒ©ãƒ : {list(df.columns)}")
    print(f"   è¡Œæ•°: {len(df)}")
    
    # ã‚«ãƒ©ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæŸ”è»Ÿã«å¯¾å¿œï¼‰
    column_mapping = {}
    for col in df.columns:
        col_lower = col.lower()
        if 'ãƒ«ãƒ¼ãƒ«' in col_lower and 'ã‚³ãƒ¼ãƒ‰' in col_lower:
            column_mapping[col] = 'rule_code'
        elif 'ãƒ«ãƒ¼ãƒ«' in col_lower and 'å' in col_lower:
            column_mapping[col] = 'rule_name'
        elif 'ã‚¤ãƒ™ãƒ³ãƒˆ' in col_lower or 'event' in col_lower:
            column_mapping[col] = 'target_event'
        elif 'è§’åº¦' in col_lower or 'metric' in col_lower:
            column_mapping[col] = 'target_metric'
        elif 'æ¼”ç®—å­' in col_lower or 'operator' in col_lower:
            column_mapping[col] = 'operator'
        elif 'é–¾å€¤' in col_lower or 'threshold' in col_lower or 'ã—ãã„' in col_lower:
            column_mapping[col] = 'threshold'
        elif 'é‡è¦åº¦' in col_lower or 'severity' in col_lower:
            column_mapping[col] = 'severity'
        elif 'å„ªå…ˆåº¦' in col_lower or 'priority' in col_lower:
            column_mapping[col] = 'priority'
    
    # ã‚«ãƒ©ãƒ åã‚’ãƒªãƒãƒ¼ãƒ 
    df = df.rename(columns=column_mapping)
    
    # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèª
    required_cols = ['rule_code', 'target_metric', 'operator', 'threshold', 'severity']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"âš ï¸  è­¦å‘Š: å¿…é ˆã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_cols}")
        print(f"   åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {list(df.columns)}")
    
    # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
    if 'target_metric' in df.columns:
        df['target_metric'] = df['target_metric'].apply(normalize_angle_name)
    if 'target_event' in df.columns:
        df['target_event'] = df['target_event'].apply(normalize_event_name)
    if 'operator' in df.columns:
        df['operator'] = df['operator'].apply(normalize_operator)
    
    return df

def load_and_process_csv2(csv_path: str) -> pd.DataFrame:
    """
    ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°åˆ†é¡.csv ã‚’èª­ã¿è¾¼ã¿
    
    æƒ³å®šã•ã‚Œã‚‹CSVæ§‹é€ :
    - åˆ†é¡ã‚³ãƒ¼ãƒ‰, åˆ†é¡å, èª¬æ˜ãªã©
    """
    print(f"ğŸ“– CSV2ã‚’èª­ã¿è¾¼ã¿ä¸­: {csv_path}")
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    df.columns = df.columns.str.strip().str.lower()
    print(f"   ã‚«ãƒ©ãƒ : {list(df.columns)}")
    print(f"   è¡Œæ•°: {len(df)}")
    return df

def load_and_process_csv3(csv_path: str) -> pd.DataFrame:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆç´ã¥ã‘.csv ã‚’èª­ã¿è¾¼ã¿ã€å°‚é–€å®¶ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    
    æƒ³å®šã•ã‚Œã‚‹CSVæ§‹é€ :
    - ãƒ«ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ‰, èª²é¡Œå, ç¾è±¡, åŸå› , æ”¹å–„ç­–, ãƒ‰ãƒªãƒ«å, ãƒ‰ãƒªãƒ«URL
    """
    print(f"ğŸ“– CSV3ã‚’èª­ã¿è¾¼ã¿ä¸­: {csv_path}")
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    df.columns = df.columns.str.strip().str.lower()
    
    print(f"   ã‚«ãƒ©ãƒ : {list(df.columns)}")
    print(f"   è¡Œæ•°: {len(df)}")
    
    # ã‚«ãƒ©ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°
    column_mapping = {}
    for col in df.columns:
        col_lower = col.lower()
        if 'ãƒ«ãƒ¼ãƒ«' in col_lower and 'ã‚³ãƒ¼ãƒ‰' in col_lower:
            column_mapping[col] = 'rule_code'
        elif 'èª²é¡Œ' in col_lower and 'å' in col_lower:
            column_mapping[col] = 'issue_name'
        elif 'ç¾è±¡' in col_lower or 'observation' in col_lower:
            column_mapping[col] = 'observation'
        elif 'åŸå› ' in col_lower or 'cause' in col_lower:
            column_mapping[col] = 'cause'
        elif 'æ”¹å–„' in col_lower or 'action' in col_lower:
            column_mapping[col] = 'action'
        elif 'ãƒ‰ãƒªãƒ«' in col_lower and 'å' in col_lower:
            column_mapping[col] = 'drill_name'
        elif 'ãƒ‰ãƒªãƒ«' in col_lower and ('url' in col_lower or 'ãƒªãƒ³ã‚¯' in col_lower):
            column_mapping[col] = 'drill_url'
        elif 'å‚™è€ƒ' in col_lower or 'notes' in col_lower:
            column_mapping[col] = 'additional_notes'
    
    df = df.rename(columns=column_mapping)
    
    return df

# ==========================================
# 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æŠ•å…¥
# ==========================================

def insert_diagnosis_rules(conn, df: pd.DataFrame):
    """è¨ºæ–­ãƒ«ãƒ¼ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŠ•å…¥"""
    cursor = conn.cursor()
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    cursor.execute("DELETE FROM diagnosis_rules;")
    
    inserted_count = 0
    for _, row in df.iterrows():
        try:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            if pd.isna(row.get('rule_code')) or pd.isna(row.get('target_metric')):
                print(f"âš ï¸  ã‚¹ã‚­ãƒƒãƒ—: å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸è¶³ã—ã¦ã„ã¾ã™ - {row.to_dict()}")
                continue
            
            cursor.execute("""
                INSERT INTO diagnosis_rules (
                    rule_code, rule_name, target_event, target_metric,
                    operator, threshold, severity, priority, is_active
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (rule_code) DO UPDATE SET
                    rule_name = EXCLUDED.rule_name,
                    target_event = EXCLUDED.target_event,
                    target_metric = EXCLUDED.target_metric,
                    operator = EXCLUDED.operator,
                    threshold = EXCLUDED.threshold,
                    severity = EXCLUDED.severity,
                    priority = EXCLUDED.priority,
                    updated_at = NOW();
            """, (
                str(row.get('rule_code', '')).strip(),
                str(row.get('rule_name', '')).strip() if not pd.isna(row.get('rule_name')) else '',
                normalize_event_name(row.get('target_event')) if not pd.isna(row.get('target_event')) else None,
                normalize_angle_name(row.get('target_metric', '')),
                normalize_operator(row.get('operator', 'lt')) if not pd.isna(row.get('operator')) else 'lt',
                float(row.get('threshold', 0)) if not pd.isna(row.get('threshold')) else 0.0,
                str(row.get('severity', 'medium')).strip().lower() if not pd.isna(row.get('severity')) else 'medium',
                int(row.get('priority', 0)) if not pd.isna(row.get('priority')) else 0,
                True
            ))
            inserted_count += 1
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: è¡Œã®æŠ•å…¥ã«å¤±æ•— - {e}")
            print(f"   ãƒ‡ãƒ¼ã‚¿: {row.to_dict()}")
            continue
    
    conn.commit()
    cursor.close()
    print(f"âœ… è¨ºæ–­ãƒ«ãƒ¼ãƒ« {inserted_count} ä»¶ã‚’æŠ•å…¥ã—ã¾ã—ãŸ")

def insert_expert_advice(conn, df: pd.DataFrame):
    """å°‚é–€å®¶ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŠ•å…¥"""
    cursor = conn.cursor()
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    cursor.execute("DELETE FROM expert_advice;")
    
    inserted_count = 0
    for _, row in df.iterrows():
        try:
            if pd.isna(row.get('rule_code')):
                print(f"âš ï¸  ã‚¹ã‚­ãƒƒãƒ—: rule_codeãŒä¸è¶³ã—ã¦ã„ã¾ã™ - {row.to_dict()}")
                continue
            
            cursor.execute("""
                INSERT INTO expert_advice (
                    rule_code, issue_name, observation, cause, action,
                    drill_name, drill_url, additional_notes
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT DO NOTHING;
            """, (
                str(row.get('rule_code', '')).strip(),
                str(row.get('issue_name', '')).strip() if not pd.isna(row.get('issue_name')) else '',
                str(row.get('observation', '')).strip() if not pd.isna(row.get('observation')) else None,
                str(row.get('cause', '')).strip() if not pd.isna(row.get('cause')) else None,
                str(row.get('action', '')).strip() if not pd.isna(row.get('action')) else None,
                str(row.get('drill_name', '')).strip() if not pd.isna(row.get('drill_name')) else None,
                str(row.get('drill_url', '')).strip() if not pd.isna(row.get('drill_url')) else None,
                str(row.get('additional_notes', '')).strip() if not pd.isna(row.get('additional_notes')) else None,
            ))
            inserted_count += 1
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: è¡Œã®æŠ•å…¥ã«å¤±æ•— - {e}")
            print(f"   ãƒ‡ãƒ¼ã‚¿: {row.to_dict()}")
            continue
    
    conn.commit()
    cursor.close()
    print(f"âœ… å°‚é–€å®¶ã‚¢ãƒ‰ãƒã‚¤ã‚¹ {inserted_count} ä»¶ã‚’æŠ•å…¥ã—ã¾ã—ãŸ")

# ==========================================
# 6. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================

def main():
    parser = argparse.ArgumentParser(description='CSVãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLã«ç§»è¡Œ')
    parser.add_argument('--csv-dir', type=str, required=True, help='CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--csv1', type=str, default='çµ±è¨ˆãƒ¢ãƒ‡ãƒ«æ¤œè¨.csv', help='è¨ºæ–­ãƒ«ãƒ¼ãƒ«CSVãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--csv2', type=str, default='ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°åˆ†é¡.csv', help='ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°åˆ†é¡CSVãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--csv3', type=str, default='ã‚³ãƒ¡ãƒ³ãƒˆç´ã¥ã‘.csv', help='ã‚³ãƒ¡ãƒ³ãƒˆç´ã¥ã‘CSVãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--skip-csv2', action='store_true', help='CSV2ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœªä½¿ç”¨ã®å ´åˆï¼‰')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸš€ CSVãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
    print("=" * 80)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    conn = get_db_connection()
    
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        create_tables(conn)
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        csv1_path = os.path.join(args.csv_dir, args.csv1)
        csv2_path = os.path.join(args.csv_dir, args.csv2)
        csv3_path = os.path.join(args.csv_dir, args.csv3)
        
        # CSV1: è¨ºæ–­ãƒ«ãƒ¼ãƒ«
        if os.path.exists(csv1_path):
            df1 = load_and_process_csv1(csv1_path)
            insert_diagnosis_rules(conn, df1)
        else:
            print(f"âš ï¸  CSV1ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv1_path}")
        
        # CSV2: ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°åˆ†é¡ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if not args.skip_csv2 and os.path.exists(csv2_path):
            df2 = load_and_process_csv2(csv2_path)
            print(f"ğŸ“Š CSV2ãƒ‡ãƒ¼ã‚¿: {len(df2)} è¡Œï¼ˆç¾åœ¨ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ï¼‰")
        
        # CSV3: å°‚é–€å®¶ã‚¢ãƒ‰ãƒã‚¤ã‚¹
        if os.path.exists(csv3_path):
            df3 = load_and_process_csv3(csv3_path)
            insert_expert_advice(conn, df3)
        else:
            print(f"âš ï¸  CSV3ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv3_path}")
        
        print("=" * 80)
        print("âœ… ç§»è¡Œå®Œäº†!")
        print("=" * 80)
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        conn.rollback()
    finally:
        conn.close()

if __name__ == "__main__":
    main()

